export interface WorkshopStep {
  time: string;
  title: string;
  description: string;
  details: string[];
}

export interface Workshop {
  slug: string;
  badge: string;
  badgeColor: string;
  title: string;
  subtitle: string;
  tagline: string;
  description: string;
  audience: string;
  level: string;
  levelColor: string;
  keyValue: string;
  wowFactor: string;
  icon: string;
  whatYouBuild: string[];
  whatWeCover: string[];
  whoIsThisFor: string;
  prerequisites: string[];
  youLeaveWith: string[];
  schedule: WorkshopStep[];
  guideSteps: GuideStep[];
}

export interface GuideStep {
  stepNumber: number;
  title: string;
  duration: string;
  overview: string;
  instructions: string[];
  commands?: string[];
  tips?: string[];
  checkpoint: string;
}

export const workshops: Workshop[] = [
  {
    slug: "local-first",
    badge: "Workshop",
    badgeColor: "badge-lime",
    title: "Local-First OpenClaw + Nebius Token Factory",
    subtitle: "Zero-Retention Inference. Open Models. No Lock-In.",
    tagline: "Your data isn't stored. Your models aren't locked. Your costs aren't hidden.",
    description:
      "Set up OpenClaw powered by open-weight models served through Nebius Token Factory — with zero-retention inference, meaning your prompts and responses are never stored at rest. You choose the model, you control the costs, and you can swap providers anytime.",
    audience: "SMBs, solo founders, cost-conscious builders",
    level: "Beginner",
    levelColor: "level-green",
    keyValue: "Zero data retention, no lock-in, cost control",
    wowFactor: "\"I switched models in 30 seconds and cut my API bill by 80%\"",
    icon: "shield",
    whatYouBuild: [
      "A fully local OpenClaw agent stack running on your laptop or Mac mini",
      "Connected to Nebius Token Factory for inference (Llama, Mistral, DeepSeek)",
      "A working business workflow running entirely under your control",
    ],
    whatWeCover: [
      "How Nebius Token Factory works: zero-retention inference, dedicated endpoints, open models",
      "Setting up OpenClaw to point at Nebius endpoints instead of closed APIs",
      "Why zero-retention matters: your prompts and responses are never stored at rest",
      "How to swap models in and out without changing your workflows",
      "Real cost comparisons: running an agent 24/7 on Token Factory vs. GPT-4 / Claude API",
    ],
    whoIsThisFor:
      "Founders, operators, and technical builders who want AI agents without vendor lock-in or unpredictable API bills. Especially relevant if you work with sensitive data (legal, finance, healthcare, HR).",
    prerequisites: [
      "Laptop that can browse the internet (WiFi may be unstable, personal hotspot may be necessary)",
      "If you have a Mac mini, you can bring it on site — we have a separate station",
      "No technical background required",
    ],
    youLeaveWith: [
      "A working local agent setup",
      "A Nebius account with Token Factory access",
      "A clear picture of what it costs to run agents on your own terms",
    ],
    schedule: [
      {
        time: "2:30 PM – 3:00 PM",
        title: "Why Local-First Matters",
        description: "Nebius Token Factory overview and zero-retention inference",
        details: [
          "Introduction to the vendor lock-in vs. open-model tradeoff",
          "How Token Factory provides API-level simplicity with open-model freedom",
          "Cost breakdown: per-token pricing vs. closed API alternatives",
        ],
      },
      {
        time: "3:00 PM – 3:45 PM",
        title: "Hands-On: OpenClaw + Token Factory Setup",
        description: "Install and configure your local agent stack",
        details: [
          "Install OpenClaw locally",
          "Create your Nebius Token Factory account and generate API keys",
          "Configure OpenClaw to use Nebius endpoints",
          "Verify your first agent call works end-to-end",
        ],
      },
      {
        time: "3:45 PM – 4:30 PM",
        title: "Build Your First Workflow",
        description: "Choose a business use case and build it live",
        details: [
          "Select from: content generation, lead qualification, research, data analysis",
          "Wire up your workflow with OpenClaw's template system",
          "Connect to Slack, Discord, or WhatsApp for output delivery",
          "Test the full loop: trigger → agent reasoning → output",
        ],
      },
      {
        time: "4:30 PM – 5:00 PM",
        title: "Cost Optimization & Group Discussion",
        description: "Strategies for running agents affordably at scale",
        details: [
          "Model selection strategies: when to use Llama vs. Mistral vs. DeepSeek",
          "Caching and batching to reduce token spend",
          "Open Q&A and industry-specific breakout discussion",
        ],
      },
    ],
    guideSteps: [
      {
        stepNumber: 1,
        title: "Set Up Your Environment",
        duration: "10 min",
        overview: "Install the required software and verify your system is ready.",
        instructions: [
          "Ensure you have Node.js 18+ installed (run node --version to check)",
          "Ensure you have Python 3.10+ installed (run python3 --version to check)",
          "Install Git if not already available (run git --version to check)",
          "Open your terminal (Terminal on Mac, PowerShell on Windows)",
        ],
        commands: [
          "# Check prerequisites",
          "node --version",
          "python3 --version",
          "git --version",
        ],
        tips: [
          "If you don't have Node.js, download it from nodejs.org",
          "Mac users can install missing tools via Homebrew: brew install node python git",
        ],
        checkpoint: "All three version commands return valid version numbers.",
      },
      {
        stepNumber: 2,
        title: "Install OpenClaw",
        duration: "10 min",
        overview: "Clone the OpenClaw repository and install dependencies.",
        instructions: [
          "Clone the OpenClaw repository from GitHub",
          "Navigate into the project directory",
          "Install dependencies using npm",
          "Verify the installation by running the health check",
        ],
        commands: [
          "git clone https://github.com/openclaw/openclaw.git",
          "cd openclaw",
          "npm install",
          "npm run health-check",
        ],
        tips: [
          "If npm install fails, try deleting node_modules and running npm install again",
          "On corporate networks, you may need to configure npm proxy settings",
        ],
        checkpoint: "The health check command returns 'OpenClaw is ready' with a green status.",
      },
      {
        stepNumber: 3,
        title: "Create Your Nebius Token Factory Account",
        duration: "10 min",
        overview: "Sign up for Nebius AI Cloud and generate your Token Factory API keys.",
        instructions: [
          "Navigate to console.nebius.com in your browser",
          "Create a new account or sign in with an existing one",
          "Navigate to Token Factory in the left sidebar",
          "Click 'Create API Key' and copy the key to your clipboard",
          "Save the key somewhere safe — you will not be able to see it again",
        ],
        tips: [
          "Nebius provides free credits for new accounts — no credit card required to start",
          "Label your API key descriptively (e.g., 'openclaw-workshop-local')",
        ],
        checkpoint: "You have a Nebius API key copied and saved securely.",
      },
      {
        stepNumber: 4,
        title: "Configure OpenClaw for Token Factory",
        duration: "10 min",
        overview: "Point OpenClaw at Nebius Token Factory endpoints instead of default APIs.",
        instructions: [
          "Copy the example environment file to create your local config",
          "Open the .env file in your text editor",
          "Set the INFERENCE_PROVIDER to 'nebius'",
          "Paste your Nebius API key into the NEBIUS_API_KEY field",
          "Choose your model (we recommend starting with meta-llama/Meta-Llama-3.1-70B-Instruct)",
          "Save the file",
        ],
        commands: [
          "cp .env.example .env",
          "# Edit .env with your preferred editor:",
          "# INFERENCE_PROVIDER=nebius",
          "# NEBIUS_API_KEY=your_key_here",
          "# NEBIUS_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct",
        ],
        tips: [
          "Never commit your .env file to version control",
          "You can switch models later without changing any workflow logic",
        ],
        checkpoint: "Running npm run test-connection returns 'Connected to Nebius Token Factory' with your model name.",
      },
      {
        stepNumber: 5,
        title: "Run Your First Agent",
        duration: "5 min",
        overview: "Start OpenClaw and verify your agent can reason and respond.",
        instructions: [
          "Start the OpenClaw server in local mode",
          "Open the OpenClaw dashboard in your browser (http://localhost:3000)",
          "Navigate to the Agent Console",
          "Type a test prompt: 'Summarize the top 3 benefits of local-first AI deployment'",
          "Verify the response comes back from your Nebius-hosted model",
        ],
        commands: [
          "npm run start:local",
          "# Open http://localhost:3000 in your browser",
        ],
        checkpoint: "The agent responds with a coherent summary. The dashboard shows 'Nebius Token Factory' as the inference provider.",
      },
      {
        stepNumber: 6,
        title: "Build Your Business Workflow",
        duration: "30 min",
        overview: "Choose a use case and wire up a complete workflow template.",
        instructions: [
          "From the dashboard, click 'New Workflow'",
          "Select a template that matches your business need:",
          "  → Content Generation: automated blog posts, social media, SEO content",
          "  → Lead Qualification: score and route incoming leads based on criteria",
          "  → Market Research: monitor competitors and summarize findings",
          "  → Data Analysis: ingest CSV/spreadsheet data and generate reports",
          "Customize the template with your specific inputs and output destinations",
          "Connect an output channel (Slack, Discord, WhatsApp, or email)",
          "Run the workflow manually to test it end-to-end",
          "Enable the scheduled trigger to run automatically",
        ],
        tips: [
          "Start with the simplest version of your workflow — you can add complexity later",
          "Test with small sample data before pointing at production sources",
          "The workflow YAML is human-readable — open it in a text editor to understand the structure",
        ],
        checkpoint: "Your workflow runs successfully, processes sample data, and delivers output to your chosen channel.",
      },
      {
        stepNumber: 7,
        title: "Cost Analysis & Optimization",
        duration: "15 min",
        overview: "Review your token usage and optimize for production cost efficiency.",
        instructions: [
          "Open the Nebius console and navigate to Token Factory > Usage",
          "Review how many tokens your test workflow consumed",
          "Calculate your estimated monthly cost at your expected run frequency",
          "Try switching to a smaller model (e.g., Llama 3.1 8B) for simpler tasks",
          "Compare the cost vs. quality tradeoff",
          "Set up usage alerts in the Nebius console to avoid surprise bills",
        ],
        tips: [
          "Smaller models are significantly cheaper and faster for routine tasks",
          "Use the large model for complex reasoning, small model for formatting and extraction",
          "Batch similar requests to maximize throughput per token",
        ],
        checkpoint: "You have a cost estimate for your use case and know which model tier to use for each task type.",
      },
    ],
  },
  {
    slug: "h200-gpu",
    badge: "Workshop",
    badgeColor: "badge-blue",
    title: "OpenClaw on Dedicated NVIDIA H200 GPU",
    subtitle: "Give Your Agents Their Own GPU. No Sharing. No Throttling.",
    tagline: "Maximum performance for high-throughput agent workloads",
    description:
      "Set up OpenClaw on dedicated NVIDIA H200 GPU infrastructure from Nebius AI Cloud. Your agents run on hardware that belongs to your workload alone.",
    audience: "Growth-stage teams, high-throughput workloads",
    level: "Intermediate",
    levelColor: "level-blue",
    keyValue: "Raw performance, no rate limits",
    wowFactor: "\"It processed 10k leads in 8 minutes\"",
    icon: "gpu",
    whatYouBuild: [
      "An OpenClaw agent running on a Nebius VM with a dedicated NVIDIA H200",
      "A self-hosted open model (Llama 3, DeepSeek, Mixtral) served via vLLM or TGI",
      "A high-throughput workflow that would be impractical on per-token APIs",
    ],
    whatWeCover: [
      "Spinning up a Nebius AI Cloud VM with H200 GPU (live walkthrough)",
      "Deploying an open model on your own GPU and connecting OpenClaw",
      "Performance benchmarks: tokens/sec, concurrent agents, sustained throughput",
      "When dedicated GPU makes financial sense vs. Token Factory vs. closed APIs",
      "Keeping your GPU utilized 24/7 to maximize cost efficiency",
      "Multi-agent architectures: running several OpenClaw agents on a single GPU",
    ],
    whoIsThisFor:
      "Builders and founders who have outgrown API rate limits. Teams running production agent workloads that need predictable performance and cost.",
    prerequisites: [
      "Laptop with SSH client (Terminal on Mac, PuTTY or WSL on Windows)",
      "Basic familiarity with command line / terminal",
      "A Nebius AI Cloud account (we'll provide setup instructions beforehand)",
    ],
    youLeaveWith: [
      "A running Nebius VM with a dedicated H200",
      "A deployed open model",
      "A working high-throughput OpenClaw pipeline",
      "A cost model for your specific workload",
    ],
    schedule: [
      {
        time: "2:30 PM – 3:00 PM",
        title: "Why Dedicated GPUs",
        description: "Nebius AI Cloud architecture and the H200 advantage",
        details: [
          "API rate limits and why they break at scale",
          "H200 specs: 141GB HBM3e, what that means for model serving",
          "Nebius pricing vs. hyperscaler alternatives",
        ],
      },
      {
        time: "3:00 PM – 3:45 PM",
        title: "Provision Your H200 VM & Deploy a Model",
        description: "Spin up GPU infrastructure and serve an open model",
        details: [
          "Create a Nebius VM with H200 GPU allocation",
          "SSH into your instance and install vLLM",
          "Download and deploy Llama 3.1 70B (or your model of choice)",
          "Verify inference is running with a test query",
        ],
      },
      {
        time: "3:45 PM – 4:30 PM",
        title: "Connect OpenClaw & Run High-Throughput Workflow",
        description: "Wire up OpenClaw to your GPU and stress test it",
        details: [
          "Point OpenClaw at your self-hosted model endpoint",
          "Build a batch processing workflow (e.g., 1000 lead scoring operations)",
          "Run the batch and measure throughput vs. API-based approach",
          "Monitor GPU utilization during the run",
        ],
      },
      {
        time: "4:30 PM – 5:00 PM",
        title: "Economics, Multi-Agent Patterns & Q&A",
        description: "Understand the cost model and advanced architectures",
        details: [
          "Calculate your break-even point: when GPU beats per-token APIs",
          "Multi-agent setups on a single GPU: how to share capacity",
          "Auto-scaling strategies for variable workloads",
          "Open Q&A",
        ],
      },
    ],
    guideSteps: [
      {
        stepNumber: 1,
        title: "Set Up Your Nebius AI Cloud Account",
        duration: "10 min",
        overview: "Create your Nebius account and configure SSH access for GPU VMs.",
        instructions: [
          "Navigate to console.nebius.com and create an account (or sign in)",
          "Complete account verification if prompted",
          "Navigate to Settings > SSH Keys",
          "Generate an SSH key pair if you don't have one, or upload your public key",
          "Note your project ID from the dashboard — you'll need it to create VMs",
        ],
        commands: [
          "# Generate SSH key if needed (Mac/Linux):",
          "ssh-keygen -t ed25519 -C 'openclaw-workshop'",
          "cat ~/.ssh/id_ed25519.pub",
          "# Copy the output and paste it into the Nebius console",
        ],
        tips: [
          "If you already have an SSH key, skip the generation step",
          "Ed25519 keys are preferred over RSA for modern setups",
        ],
        checkpoint: "Your SSH public key appears in the Nebius console under Settings > SSH Keys.",
      },
      {
        stepNumber: 2,
        title: "Provision an H200 GPU VM",
        duration: "15 min",
        overview: "Spin up a virtual machine with a dedicated NVIDIA H200 GPU.",
        instructions: [
          "In the Nebius console, navigate to Compute > Virtual Machines",
          "Click 'Create VM'",
          "Select the GPU-optimized platform (gpu-h200)",
          "Choose 1x NVIDIA H200 (141GB HBM3e)",
          "Select Ubuntu 22.04 with CUDA pre-installed as the OS image",
          "Set CPU to 16 cores and RAM to 64GB (recommended minimum)",
          "Set disk to 200GB SSD (models require significant storage)",
          "Select your SSH key for access",
          "Click 'Create' and wait for the VM to reach 'Running' status",
          "Copy the public IP address from the VM details page",
        ],
        tips: [
          "The VM takes 2-3 minutes to fully boot and become SSH-accessible",
          "200GB disk is sufficient for most 70B parameter models",
          "You can stop the VM when not in use to save costs",
        ],
        checkpoint: "VM status shows 'Running' and you have the public IP address copied.",
      },
      {
        stepNumber: 3,
        title: "SSH In and Install vLLM",
        duration: "15 min",
        overview: "Connect to your GPU VM and set up the model serving infrastructure.",
        instructions: [
          "SSH into your Nebius VM using the IP address from the previous step",
          "Verify the GPU is detected by running nvidia-smi",
          "Install vLLM (the high-performance inference server)",
          "Verify vLLM installed correctly",
        ],
        commands: [
          "ssh ubuntu@YOUR_VM_IP",
          "nvidia-smi  # Should show H200 with 141GB memory",
          "pip install vllm",
          "python -c \"import vllm; print(vllm.__version__)\"",
        ],
        tips: [
          "If nvidia-smi doesn't show the GPU, the CUDA drivers may need a reboot: sudo reboot",
          "vLLM installation takes a few minutes — this is normal",
        ],
        checkpoint: "nvidia-smi shows the H200 GPU and vLLM version prints successfully.",
      },
      {
        stepNumber: 4,
        title: "Deploy an Open Model",
        duration: "15 min",
        overview: "Download and serve a large language model on your dedicated GPU.",
        instructions: [
          "Start vLLM with Llama 3.1 70B Instruct (or your preferred model)",
          "The model will download on first run (~40GB, takes a few minutes on Nebius network)",
          "Wait for the server to print 'Uvicorn running on http://0.0.0.0:8000'",
          "Test the endpoint with a curl command from the VM",
        ],
        commands: [
          "# Start the model server (run in a tmux/screen session for persistence)",
          "tmux new -s model-server",
          "python -m vllm.entrypoints.openai.api_server \\",
          "  --model meta-llama/Meta-Llama-3.1-70B-Instruct \\",
          "  --host 0.0.0.0 \\",
          "  --port 8000 \\",
          "  --tensor-parallel-size 1",
          "",
          "# Test from another terminal (Ctrl-B then C in tmux for new window):",
          "curl http://localhost:8000/v1/completions \\",
          "  -H 'Content-Type: application/json' \\",
          "  -d '{\"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\", \"prompt\": \"Hello\", \"max_tokens\": 50}'",
        ],
        tips: [
          "Use tmux so the model server persists if your SSH connection drops",
          "The 70B model fits comfortably in 141GB H200 memory with room for KV cache",
          "For faster iteration during the workshop, try the 8B model first",
        ],
        checkpoint: "The curl test returns a valid completion response from your self-hosted model.",
      },
      {
        stepNumber: 5,
        title: "Connect OpenClaw to Your GPU",
        duration: "10 min",
        overview: "Configure OpenClaw to use your self-hosted model instead of external APIs.",
        instructions: [
          "On your local machine, open your OpenClaw installation",
          "Edit the .env file to point at your GPU VM",
          "Set the inference provider to 'openai-compatible' (vLLM serves an OpenAI-compatible API)",
          "Set the base URL to your VM's public IP on port 8000",
          "Start OpenClaw and test the connection",
        ],
        commands: [
          "# In your OpenClaw .env file:",
          "# INFERENCE_PROVIDER=openai-compatible",
          "# OPENAI_BASE_URL=http://YOUR_VM_IP:8000/v1",
          "# OPENAI_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct",
          "# OPENAI_API_KEY=none  (vLLM doesn't require auth by default)",
          "",
          "npm run test-connection",
        ],
        tips: [
          "Ensure your Nebius VM's security group allows inbound traffic on port 8000",
          "For production, add authentication to vLLM and use HTTPS",
        ],
        checkpoint: "OpenClaw test-connection confirms it's talking to your self-hosted model on the H200.",
      },
      {
        stepNumber: 6,
        title: "Run a High-Throughput Batch Workflow",
        duration: "20 min",
        overview: "Build and run a workflow that demonstrates the power of dedicated GPU compute.",
        instructions: [
          "Open the OpenClaw dashboard",
          "Create a new workflow using the 'Batch Processing' template",
          "Configure it for your use case (e.g., score 1000 leads, generate 500 product descriptions)",
          "Prepare a sample CSV with at least 100 rows of test data",
          "Upload the CSV and run the batch workflow",
          "Monitor GPU utilization during the run: watch nvidia-smi on the VM",
          "Record the total time and calculate throughput (items/minute)",
          "Compare this to estimated time using per-token APIs at their rate limits",
        ],
        tips: [
          "The H200 should sustain 2000-4000 tokens/sec for Llama 3.1 70B",
          "Run multiple concurrent requests to maximize GPU utilization",
          "Use 'watch -n 1 nvidia-smi' to see real-time GPU memory and compute usage",
        ],
        checkpoint: "Batch completes successfully. You have throughput numbers and a direct comparison to API-based performance.",
      },
      {
        stepNumber: 7,
        title: "Cost Analysis & Multi-Agent Architecture",
        duration: "15 min",
        overview: "Calculate your break-even point and explore advanced patterns.",
        instructions: [
          "Check your Nebius billing dashboard for the VM cost during the workshop",
          "Calculate monthly cost: VM hourly rate × 24 × 30",
          "Calculate equivalent API cost: total tokens used × per-token rate from GPT-4/Claude",
          "Find your break-even: at what volume does dedicated GPU become cheaper?",
          "Explore multi-agent setup: configure multiple OpenClaw agents hitting the same vLLM server",
          "Test concurrent agent execution to verify no throttling occurs",
        ],
        tips: [
          "Most teams break even at 10-50M tokens/month depending on the model",
          "You can run 3-5 concurrent OpenClaw agents on a single H200 for Llama 70B",
          "When not running batch jobs, use the GPU for other agent tasks to maximize utilization",
        ],
        checkpoint: "You have a cost model showing your break-even point and know whether dedicated GPU makes sense for your workload.",
      },
    ],
  },
  {
    slug: "kubernetes",
    badge: "Workshop",
    badgeColor: "badge-cyan",
    title: "OpenClaw on Kubernetes",
    subtitle: "Production-Grade Agent Infrastructure",
    tagline: "Deploy agents like you deploy software. Scale them the same way.",
    description:
      "Take OpenClaw from 'cool tool' to production system by deploying it on Kubernetes with Nebius Managed Kubernetes for orchestration.",
    audience: "Engineering teams, platform builders",
    level: "Advanced",
    levelColor: "level-orange",
    keyValue: "Scale, reliability, production ops",
    wowFactor: "\"50 agents, zero downtime\"",
    icon: "kubernetes",
    whatYouBuild: [
      "An OpenClaw deployment running on Nebius Managed Kubernetes",
      "Auto-scaling agent pools that spin up and down based on workload",
      "A multi-agent setup with different agents for different business functions",
      "Monitoring and logging for full observability",
    ],
    whatWeCover: [
      "Containerizing OpenClaw and deploying to Kubernetes (Helm charts, manifests)",
      "Nebius Managed Kubernetes vs. self-managed K8s",
      "Horizontal pod autoscaling for agent workloads, GPU node pools",
      "Connecting agents to shared resources safely",
      "Secrets management, networking, and security basics",
      "CI/CD for agents: updating workflows without downtime",
      "Cost management: right-sizing nodes, spot instances",
    ],
    whoIsThisFor:
      "Engineering teams, DevOps/platform engineers, and technical founders who want to run OpenClaw in production. You should be comfortable with containers and basic cloud infrastructure.",
    prerequisites: [
      "Laptop with kubectl installed",
      "Basic familiarity with Docker and Kubernetes concepts",
      "A Nebius AI Cloud account (setup instructions provided beforehand)",
      "Docker Desktop or equivalent container runtime installed",
    ],
    youLeaveWith: [
      "A running Kubernetes cluster on Nebius with OpenClaw deployed",
      "Helm charts you can reuse",
      "A scaling configuration for your workload",
      "An architecture pattern for running agents in production",
    ],
    schedule: [
      {
        time: "2:30 PM – 3:00 PM",
        title: "Architecture Overview: Agents as Microservices",
        description: "Why Kubernetes for agents and the target architecture",
        details: [
          "From single-agent to multi-agent: when you need orchestration",
          "OpenClaw as a containerized service: the deployment model",
          "Nebius Managed Kubernetes: what it handles for you",
        ],
      },
      {
        time: "3:00 PM – 4:00 PM",
        title: "Deploy OpenClaw on Nebius Managed K8s",
        description: "Hands-on cluster creation and deployment",
        details: [
          "Create a Nebius Managed Kubernetes cluster",
          "Build and push the OpenClaw container image",
          "Deploy using Helm charts with environment-specific values",
          "Verify pods are running and healthy",
        ],
      },
      {
        time: "4:00 PM – 4:30 PM",
        title: "Scaling, Monitoring & Multi-Agent Patterns",
        description: "Production-ready configuration and observability",
        details: [
          "Configure Horizontal Pod Autoscaler for agent workloads",
          "Set up basic monitoring with resource metrics",
          "Deploy multiple agent types as separate services",
          "Inter-agent communication patterns",
        ],
      },
      {
        time: "4:30 PM – 5:00 PM",
        title: "Production Hardening & Architecture Review",
        description: "Security, reliability, and group architecture review",
        details: [
          "Secrets management with Kubernetes secrets",
          "Network policies for agent isolation",
          "Rolling update strategy for zero-downtime deploys",
          "Group review: present your architecture, get feedback",
        ],
      },
    ],
    guideSteps: [
      {
        stepNumber: 1,
        title: "Verify Prerequisites",
        duration: "10 min",
        overview: "Ensure all required tools are installed and configured.",
        instructions: [
          "Verify Docker is installed and running",
          "Verify kubectl is installed",
          "Verify Helm 3 is installed",
          "Log into the Nebius CLI (or install it if needed)",
          "Verify you can access the Nebius console",
        ],
        commands: [
          "docker --version",
          "kubectl version --client",
          "helm version",
          "# Install Nebius CLI if needed:",
          "# curl -sSL https://storage.ai.nebius.cloud/cli/install.sh | bash",
          "nebius iam whoami",
        ],
        tips: [
          "Docker Desktop on Mac may need to be started manually from Applications",
          "If helm is not installed: brew install helm (Mac) or see helm.sh/docs/intro/install",
        ],
        checkpoint: "All four version commands succeed. nebius iam whoami shows your account.",
      },
      {
        stepNumber: 2,
        title: "Create a Nebius Managed Kubernetes Cluster",
        duration: "15 min",
        overview: "Provision a production-ready Kubernetes cluster on Nebius AI Cloud.",
        instructions: [
          "In the Nebius console, navigate to Managed Kubernetes",
          "Click 'Create Cluster'",
          "Name your cluster 'openclaw-production'",
          "Select the region closest to your users",
          "Configure the default node pool: 3 nodes, 4 vCPU / 16GB RAM each",
          "Enable auto-scaling (min: 2, max: 6 nodes)",
          "Click 'Create' and wait for the cluster to reach 'Running' status (3-5 min)",
          "Download the kubeconfig file from the cluster details page",
          "Configure kubectl to use your new cluster",
        ],
        commands: [
          "# After downloading kubeconfig:",
          "export KUBECONFIG=~/Downloads/openclaw-production-kubeconfig.yaml",
          "kubectl get nodes  # Should show 3 ready nodes",
          "kubectl cluster-info",
        ],
        tips: [
          "The cluster creation takes 3-5 minutes — use this time to review the Helm chart in the next step",
          "3 nodes gives you high availability; you can scale down to 2 for cost savings",
        ],
        checkpoint: "kubectl get nodes shows 3 nodes in Ready status.",
      },
      {
        stepNumber: 3,
        title: "Containerize OpenClaw",
        duration: "10 min",
        overview: "Build a Docker image for OpenClaw and push it to a container registry.",
        instructions: [
          "Navigate to your OpenClaw project directory",
          "Review the included Dockerfile (or create one)",
          "Build the Docker image with a version tag",
          "Push to the Nebius Container Registry (or Docker Hub)",
        ],
        commands: [
          "cd openclaw",
          "docker build -t openclaw:v1.0.0 .",
          "# Tag for Nebius Container Registry:",
          "docker tag openclaw:v1.0.0 cr.nebius.cloud/YOUR_PROJECT_ID/openclaw:v1.0.0",
          "# Push:",
          "docker push cr.nebius.cloud/YOUR_PROJECT_ID/openclaw:v1.0.0",
        ],
        tips: [
          "If using Docker Hub instead: docker tag openclaw:v1.0.0 yourusername/openclaw:v1.0.0",
          "Multi-stage builds keep the final image small — check the Dockerfile for this pattern",
        ],
        checkpoint: "docker images shows your openclaw:v1.0.0 image. Push completes without errors.",
      },
      {
        stepNumber: 4,
        title: "Deploy with Helm",
        duration: "15 min",
        overview: "Use Helm charts to deploy OpenClaw to your Kubernetes cluster.",
        instructions: [
          "Review the OpenClaw Helm chart structure (chart.yaml, values.yaml, templates/)",
          "Create a values-production.yaml override file for your environment",
          "Set the container image, replica count, and resource limits",
          "Configure environment variables (inference provider, API keys) as Kubernetes secrets",
          "Deploy with Helm",
          "Verify pods are running",
        ],
        commands: [
          "# Create the secret for API keys:",
          "kubectl create secret generic openclaw-secrets \\",
          "  --from-literal=NEBIUS_API_KEY=your_key_here \\",
          "  --from-literal=SLACK_TOKEN=your_slack_token",
          "",
          "# Deploy with Helm:",
          "helm install openclaw ./charts/openclaw \\",
          "  -f values-production.yaml \\",
          "  --set image.repository=cr.nebius.cloud/YOUR_PROJECT_ID/openclaw \\",
          "  --set image.tag=v1.0.0 \\",
          "  --set replicaCount=3",
          "",
          "# Verify:",
          "kubectl get pods -l app=openclaw",
          "kubectl get svc openclaw",
        ],
        tips: [
          "Always use Kubernetes secrets for API keys — never hardcode them in values.yaml",
          "Start with 3 replicas for the workshop; scale down to 1 for development",
        ],
        checkpoint: "kubectl get pods shows 3 openclaw pods in Running status. The service has an external IP.",
      },
      {
        stepNumber: 5,
        title: "Configure Auto-Scaling",
        duration: "10 min",
        overview: "Set up Horizontal Pod Autoscaler so agents scale with demand.",
        instructions: [
          "Create a HorizontalPodAutoscaler resource for the OpenClaw deployment",
          "Set CPU threshold to 70% (scale up when agents are heavily loaded)",
          "Set minimum replicas to 2 and maximum to 10",
          "Generate load to test scaling behavior",
          "Watch pods scale up in real-time",
        ],
        commands: [
          "kubectl autoscale deployment openclaw \\",
          "  --cpu-percent=70 \\",
          "  --min=2 \\",
          "  --max=10",
          "",
          "# Watch scaling in real-time:",
          "kubectl get hpa openclaw --watch",
          "",
          "# In another terminal, generate load:",
          "# (Use the batch workflow from your OpenClaw dashboard)",
        ],
        tips: [
          "HPA checks metrics every 15 seconds by default",
          "Scale-down has a 5-minute cooldown to prevent flapping",
          "For GPU node pools, you'll need custom metrics (covered in advanced section)",
        ],
        checkpoint: "kubectl get hpa shows the autoscaler active. Under load, replica count increases automatically.",
      },
      {
        stepNumber: 6,
        title: "Deploy Multiple Agent Types",
        duration: "15 min",
        overview: "Run different agent types as separate Kubernetes services.",
        instructions: [
          "Create a second Helm release for a different agent type (e.g., 'openclaw-research')",
          "Use different values files to configure each agent's purpose and model",
          "Deploy the second agent alongside the first",
          "Verify both agent types are running independently",
          "Test that each agent handles its designated workflow type",
          "Set up inter-agent communication via Kubernetes services",
        ],
        commands: [
          "# Deploy a research-focused agent:",
          "helm install openclaw-research ./charts/openclaw \\",
          "  -f values-research.yaml \\",
          "  --set image.tag=v1.0.0 \\",
          "  --set replicaCount=2",
          "",
          "# Deploy a sales-focused agent:",
          "helm install openclaw-sales ./charts/openclaw \\",
          "  -f values-sales.yaml \\",
          "  --set image.tag=v1.0.0 \\",
          "  --set replicaCount=2",
          "",
          "# Verify all agents:",
          "kubectl get pods --show-labels",
          "kubectl get svc",
        ],
        checkpoint: "Multiple agent deployments are running. Each handles its workflow type independently.",
      },
      {
        stepNumber: 7,
        title: "Production Hardening",
        duration: "15 min",
        overview: "Add security, monitoring, and reliability configurations.",
        instructions: [
          "Apply network policies to restrict agent-to-agent communication",
          "Configure liveness and readiness probes in the Helm chart",
          "Set up a rolling update strategy for zero-downtime deploys",
          "Review resource limits and requests for each pod",
          "Test a rolling update by pushing a new image tag",
          "Verify zero downtime during the update",
        ],
        commands: [
          "# Apply network policy:",
          "kubectl apply -f network-policy.yaml",
          "",
          "# Test rolling update:",
          "helm upgrade openclaw ./charts/openclaw \\",
          "  --set image.tag=v1.0.1 \\",
          "  --set strategy.type=RollingUpdate \\",
          "  --set strategy.rollingUpdate.maxUnavailable=0 \\",
          "  --set strategy.rollingUpdate.maxSurge=1",
          "",
          "# Watch the rollout:",
          "kubectl rollout status deployment/openclaw",
        ],
        tips: [
          "maxUnavailable=0 ensures no downtime during updates",
          "Always set resource requests AND limits to prevent noisy-neighbor issues",
          "In production, add Pod Disruption Budgets to protect against node drains",
        ],
        checkpoint: "Rolling update completes with zero downtime. Network policies are applied. All probes are healthy.",
      },
    ],
  },
  {
    slug: "robotics",
    badge: "Workshop",
    badgeColor: "badge-pink",
    title: "OpenClaw + Robotics with SO-ARM100",
    subtitle: "Teach AI Agents to Control Physical Machines",
    tagline: "Your AI agent just got hands",
    description:
      "Build an OpenClaw agent that can see, reason, and act in the physical world using SO-ARM100 robotic arms and computer vision. Pick up objects, sort items, and learn from demonstration.",
    audience: "Builders, robotics curious, hardware tinkerers",
    level: "Intermediate",
    levelColor: "level-pink",
    keyValue: "Physical-world agent control",
    wowFactor: "\"The robot just sorted my parts\"",
    icon: "robot",
    whatYouBuild: [
      "An OpenClaw agent connected to a SO-ARM100 robotic arm via serial control",
      "A computer vision pipeline using a USB camera for object detection",
      "A training loop where you demonstrate tasks and the agent learns to replicate them",
      "A complete pick-and-place workflow: see → decide → act → observe → adjust",
    ],
    whatWeCover: [
      "SO-ARM100 hardware: assembly, calibration, and serial communication",
      "Connecting OpenClaw to physical actuators",
      "Computer vision: camera setup, object detection, coordinate mapping",
      "Learning from demonstration: recording movements, training a policy, replaying",
      "The agent reasoning loop: see → think → act → observe → adjust",
      "Safety and control boundaries for physical agents",
      "Where this goes: multi-arm coordination, mobile platforms, warehouse automation",
    ],
    whoIsThisFor:
      "Builders, founders, and engineers curious about where AI agents meet the physical world. Robotics hobbyists who want AI reasoning. Anyone who wants to see agents do something, not just say something. No robotics experience required.",
    prerequisites: [
      "Laptop that can run a serial terminal and Python 3.10+",
      "Curiosity about what happens when software can touch things",
      "Optional: bring your own SO-ARM100 if you have one",
    ],
    youLeaveWith: [
      "A trained pick-and-place behavior on a physical robot",
      "A working vision pipeline connected to OpenClaw",
      "Understanding of the see-think-act agent loop for physical tasks",
      "Code and configs you can replicate at home with a $300 arm kit",
    ],
    schedule: [
      {
        time: "2:30 PM – 3:00 PM",
        title: "The Agent-Robotics Stack",
        description: "How OpenClaw controls physical hardware",
        details: [
          "From digital workflows to physical actions: the conceptual bridge",
          "SO-ARM100 hardware overview: 6 DOF, servo control, workspace limits",
          "The full stack: camera → model → OpenClaw → serial → arm",
        ],
      },
      {
        time: "3:00 PM – 3:30 PM",
        title: "Station Setup",
        description: "Arm calibration, camera mounting, serial connection",
        details: [
          "Connect the SO-ARM100 to your laptop via USB serial",
          "Calibrate servo positions and verify joint range of motion",
          "Mount and position the USB camera for overhead or angled view",
          "Run the diagnostics script to confirm all hardware is communicating",
        ],
      },
      {
        time: "3:30 PM – 4:15 PM",
        title: "Train Your Robot by Demonstration",
        description: "Record human movements and build the vision pipeline",
        details: [
          "Enter teach mode: physically guide the arm through a pick-and-place task",
          "Record the trajectory (joint angles over time)",
          "Set up object detection: calibrate the camera for your workspace",
          "Map pixel coordinates to arm workspace coordinates",
          "Train a simple policy from your demonstrations",
        ],
      },
      {
        time: "4:15 PM – 4:45 PM",
        title: "Connect OpenClaw: Agent → Physical Action",
        description: "Wire the reasoning loop to the robot arm",
        details: [
          "Configure OpenClaw to use the arm as a tool",
          "Build the agent loop: camera input → model reasoning → arm command",
          "Test autonomous operation: agent decides what to pick up and where to place it",
          "Iterate: refine the behavior by adding more demonstrations",
        ],
      },
      {
        time: "4:45 PM – 5:00 PM",
        title: "Demo Showcase & What Comes Next",
        description: "Show your work and explore advanced possibilities",
        details: [
          "Each team demos their trained behavior",
          "Discussion: multi-arm coordination, sim-to-real transfer",
          "How these patterns scale to warehouse, lab, and manufacturing use cases",
        ],
      },
    ],
    guideSteps: [
      {
        stepNumber: 1,
        title: "Install Software Prerequisites",
        duration: "10 min",
        overview: "Set up Python and the required libraries for robotics control and vision.",
        instructions: [
          "Ensure Python 3.10+ is installed",
          "Create a virtual environment for the workshop",
          "Install the robotics control libraries",
          "Install computer vision dependencies",
          "Verify all imports work",
        ],
        commands: [
          "python3 --version  # Must be 3.10+",
          "python3 -m venv openclaw-robotics",
          "source openclaw-robotics/bin/activate  # Mac/Linux",
          "# .\\openclaw-robotics\\Scripts\\activate  # Windows",
          "",
          "pip install pyserial opencv-python numpy",
          "pip install openclaw-robotics  # OpenClaw robotics extension",
          "",
          "# Verify:",
          "python3 -c \"import serial, cv2, numpy; print('All dependencies ready')\"",
        ],
        tips: [
          "If pip install opencv-python fails on Mac, try: pip install opencv-python-headless",
          "On Windows, you may need to install the CH340 serial driver for the SO-ARM100",
        ],
        checkpoint: "The verification command prints 'All dependencies ready' without errors.",
      },
      {
        stepNumber: 2,
        title: "Connect & Calibrate the SO-ARM100",
        duration: "15 min",
        overview: "Physically connect the robotic arm and verify all joints respond.",
        instructions: [
          "Connect the SO-ARM100 to your laptop via the USB cable",
          "Identify the serial port the arm is connected to",
          "Run the connection test script",
          "Calibrate each joint by moving to known positions",
          "Verify the arm can reach its home position (all joints centered)",
          "Test each joint individually to confirm full range of motion",
        ],
        commands: [
          "# Find the serial port:",
          "# Mac: ls /dev/tty.usb*",
          "# Linux: ls /dev/ttyUSB*",
          "# Windows: Check Device Manager > Ports",
          "",
          "python3 -m openclaw_robotics.connect --port /dev/tty.usbserial-XXXX",
          "python3 -m openclaw_robotics.calibrate --port /dev/tty.usbserial-XXXX",
          "python3 -m openclaw_robotics.test_joints --port /dev/tty.usbserial-XXXX",
        ],
        tips: [
          "Keep the arm on a stable surface — sudden movements can tip lightweight tables",
          "If a joint doesn't respond, check the servo cable connection at the control board",
          "The gripper servo (joint 6) may need manual zeroing — follow the on-screen instructions",
        ],
        checkpoint: "All 6 joints respond. The arm moves to home position. The gripper opens and closes on command.",
      },
      {
        stepNumber: 3,
        title: "Set Up the Camera & Vision Pipeline",
        duration: "15 min",
        overview: "Mount the camera, calibrate it, and verify object detection works.",
        instructions: [
          "Mount the USB camera above the arm's workspace (overhead or 45-degree angle)",
          "Connect the camera to your laptop",
          "Run the camera test to verify the feed is working",
          "Calibrate the workspace: place the calibration card on the work surface",
          "Run the calibration script to map pixel coordinates to arm coordinates",
          "Place a test object in the workspace and verify detection",
        ],
        commands: [
          "# Test camera feed (opens a preview window):",
          "python3 -m openclaw_robotics.camera_test",
          "",
          "# Calibrate pixel-to-arm coordinate mapping:",
          "python3 -m openclaw_robotics.calibrate_vision --port /dev/tty.usbserial-XXXX",
          "# Follow the on-screen prompts: the arm will move to known positions",
          "# and the camera will record where those positions appear in the image",
          "",
          "# Test object detection:",
          "python3 -m openclaw_robotics.detect_objects",
        ],
        tips: [
          "Consistent lighting dramatically improves detection accuracy — avoid shadows",
          "The overhead camera angle is easiest for pick-and-place coordinate mapping",
          "Use high-contrast objects for the workshop (bright colored blocks work great)",
        ],
        checkpoint: "Camera shows a live feed. Calibration completes. Detection highlights objects with bounding boxes.",
      },
      {
        stepNumber: 4,
        title: "Record Demonstrations (Learning from Demonstration)",
        duration: "15 min",
        overview: "Teach the robot by physically guiding it through tasks and recording the motions.",
        instructions: [
          "Enter teach mode (servos go limp so you can move the arm by hand)",
          "Place a colored block in the workspace",
          "Physically guide the arm: approach → grasp → lift → move → place → release",
          "The system records joint angles and camera frames at each timestep",
          "Repeat the demonstration 5-10 times with slightly different object positions",
          "Review your recorded demonstrations",
        ],
        commands: [
          "# Enter teach mode and start recording:",
          "python3 -m openclaw_robotics.teach \\",
          "  --port /dev/tty.usbserial-XXXX \\",
          "  --task pick-and-place \\",
          "  --demonstrations 5",
          "",
          "# After each demonstration, press Enter to save and reset",
          "# Press Q to finish recording",
          "",
          "# Review recordings:",
          "python3 -m openclaw_robotics.review_demos --task pick-and-place",
        ],
        tips: [
          "Smooth, consistent movements produce better training data than jerky ones",
          "Vary the starting position of the object between demonstrations",
          "5-10 demos is sufficient for a basic pick-and-place policy",
          "You can delete bad demonstrations during the review step",
        ],
        checkpoint: "5+ demonstrations recorded. Review shows consistent trajectories with varied start positions.",
      },
      {
        stepNumber: 5,
        title: "Train a Policy from Demonstrations",
        duration: "10 min",
        overview: "Convert your demonstrations into a reusable behavior policy.",
        instructions: [
          "Run the training script on your recorded demonstrations",
          "The trainer learns a mapping: camera observation → arm action",
          "Training runs locally on CPU (no GPU needed for this dataset size)",
          "Review the training loss curve to verify convergence",
          "Test the trained policy in simulation before running on the real arm",
        ],
        commands: [
          "# Train the policy:",
          "python3 -m openclaw_robotics.train \\",
          "  --task pick-and-place \\",
          "  --epochs 100 \\",
          "  --output policy_pick_place.pt",
          "",
          "# Test in simulation:",
          "python3 -m openclaw_robotics.simulate \\",
          "  --policy policy_pick_place.pt \\",
          "  --episodes 5",
        ],
        tips: [
          "100 epochs on 5-10 demonstrations takes about 30 seconds on a modern laptop",
          "If the simulation shows erratic behavior, record more demonstrations and retrain",
          "The policy file is small (~5MB) and portable — you can share it with others",
        ],
        checkpoint: "Training completes. Simulation shows the virtual arm completing the pick-and-place task.",
      },
      {
        stepNumber: 6,
        title: "Connect OpenClaw to the Robot",
        duration: "15 min",
        overview: "Wire OpenClaw as the decision-making brain that controls the arm using your trained policy.",
        instructions: [
          "Start OpenClaw with the robotics plugin enabled",
          "Register the arm as a tool that OpenClaw can call",
          "Register the camera as an observation source",
          "Load your trained policy as the default behavior for pick-and-place requests",
          "Test the full loop: ask OpenClaw to 'pick up the red block and place it on the right'",
          "Watch the agent: observe camera → reason about what to do → execute arm movements",
        ],
        commands: [
          "# Start OpenClaw with robotics plugin:",
          "OPENCLAW_PLUGINS=robotics npm run start:local",
          "",
          "# In the OpenClaw dashboard, configure the robotics tool:",
          "# 1. Go to Settings > Tools > Add Tool",
          "# 2. Select 'SO-ARM100 Robotic Arm'",
          "# 3. Set serial port and load your policy file",
          "# 4. Enable camera observation source",
          "",
          "# Test from the agent console:",
          "# > Pick up the red block and place it to the right",
        ],
        tips: [
          "The agent can combine vision reasoning with arm control — try asking 'sort all blocks by color'",
          "OpenClaw decides WHAT to do; the trained policy handles HOW to move the arm",
          "You can add multiple tools: arm + camera + speaker for multi-modal agent behavior",
        ],
        checkpoint: "OpenClaw receives a natural language command, uses the camera to see, reasons about the task, and commands the arm to execute. The block moves.",
      },
      {
        stepNumber: 7,
        title: "Iterate & Explore Advanced Behaviors",
        duration: "15 min",
        overview: "Refine the system and explore what else your agent-controlled robot can do.",
        instructions: [
          "Try more complex commands: 'sort objects by size', 'stack blocks in a tower'",
          "If the arm misses, record additional demonstrations for edge cases",
          "Experiment with different camera angles and their effect on accuracy",
          "Try continuous operation: the agent monitors the workspace and acts when new objects appear",
          "Discuss with your team: how would you adapt this for your use case?",
          "Document what worked and what needs improvement",
        ],
        tips: [
          "The gap between 'demo that works once' and 'reliable system' is where the real engineering lives",
          "Sim-to-real transfer can help: train extensively in simulation, fine-tune on real hardware",
          "For production robotics, you'd add safety constraints: force limits, workspace boundaries, emergency stop",
          "The SO-ARM100 kit costs ~$300 — you can replicate this entire setup at home",
        ],
        checkpoint: "You've run multiple autonomous cycles. The agent handles at least 2 different tasks. You have a clear picture of what works and what to improve.",
      },
    ],
  },
];
